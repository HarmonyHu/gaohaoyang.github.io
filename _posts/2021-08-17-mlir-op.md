---
layout: single
title: MLIR的概念整理
categories:
  - AI
tags:
  - Mlir
---

* content
{:toc}


以一段来自官网[toy](https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/)，来解释各种概念

```python
# User defined generic function that operates on unknown shaped arguments.
def multiply_transpose(a, b) {
  return transpose(a) * transpose(b);
}

def main() {
  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];
  var b<2, 3> = [1, 2, 3, 4, 5, 6];
  var c = multiply_transpose(a, b);
  var d = multiply_transpose(b, a);
  print(d);
}
```

转成IR定义如下：

<!--more-->

```json
module {
  func @multiply_transpose(%arg0: tensor<*xf64>, %arg1: tensor<*xf64>) -> tensor<*xf64> {
    %0 = "toy.transpose"(%arg0) : (tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:10)
    %1 = "toy.transpose"(%arg1) : (tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
    %2 = "toy.mul"(%0, %1) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
    "toy.return"(%2) : (tensor<*xf64>) -> () loc("Toy/Ch2/codegen.toy":5:3)
  } loc("Toy/Ch2/codegen.toy":4:1)
  func @main() {
    %0 = "toy.constant"() {value = dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":9:17)
    %1 = "toy.reshape"(%0) : (tensor<2x3xf64>) -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":9:3)
    %2 = "toy.constant"() {value = dense<[1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]> : tensor<6xf64>} : () -> tensor<6xf64> loc("Toy/Ch2/codegen.toy":10:17)
    %3 = "toy.reshape"(%2) : (tensor<6xf64>) -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":10:3)
    %4 = "toy.generic_call"(%1, %3) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":11:11)
    %5 = "toy.generic_call"(%3, %1) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":12:11)
    "toy.print"(%5) : (tensor<*xf64>) -> () loc("Toy/Ch2/codegen.toy":13:3)
    "toy.return"() : () -> () loc("Toy/Ch2/codegen.toy":8:1)
  } loc("Toy/Ch2/codegen.toy":8:1)
} loc(unknown)
```



## mlir::Operation

```json
%2 = "toy.mul"(%0, %1) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
```

Operation是指操作，也可以理解成运算。如上中`%2 = "toy.mul"(%0,%1)`可以看做一个operation。如toy中的`toy.mul`、`toy.transpose`、`toy.constant`等等都是Operation的名称。

代码中`mlir::Operation`是通用定义，包含通用的接口和属性；`MulOp`、`TransposeOp`、`ConstantOp`等等是特定定义，包含特定的属性。前者可以通过`llvm::dyn_cast`（动态）或`llvm::cast`（静态）转换成后者；后者通过`getOperation()`转换成前者。如下：

``` c++
void processConstantOp(mlir::Operation *operation) {
  ConstantOp op = llvm::dyn_cast<ConstantOp>(operation);

  // This operation is not an instance of `ConstantOp`.
  if (!op)
    return;

  // Get the internal operation instance wrapped by the smart pointer.
  mlir::Operation *internalOperation = op.getOperation();
  assert(internalOperation == operation &&
         "these operation instances are the same");
}
```

特定Op可以直接访问属性；`mlir::Operation`无法直接访问特定Op的属性，但可以类似如下间接访问，如下：

``` c++
// getAttrOfType
IntegerAttr opsetAttr = op->getAttrOfType<::mlir::Attribute>("onnx_opset").dyn_cast_or_null<IntegerAttr>();
if (opsetAttr)
  opset = opsetAttr.getValue().getSExtValue();
// getAttr
mlir::Type targetType = op->getAttr("to").cast<::mlir::TypeAttr>().getValue();
```

`mlir::Operation`有如下常用接口：

```c++
OperationName getName() { return name; }
/// Remove this operation from its parent block and delete it.
void erase();
/// Remove the operation from its parent block, but don't delete it.
void remove();
/// Returns the operation block that contains this operation.
Block *getBlock() { return block; }
/// Return the context this operation is associated with.
MLIRContext *getContext();
void dump();
unsigned getNumOperands();
Value getOperand(unsigned idx) { return getOpOperand(idx).get(); }
void setOperand(unsigned idx, Value value);
unsigned getNumResults();
/// Get the 'idx'th result of this operation.
OpResult getResult(unsigned idx) { return OpResult(this, idx); }
void replaceAllUsesWith(Operation *op);
void moveBefore(Operation *existingOp);
void moveBefore(Block *block, llvm::iplist<Operation>::iterator iterator);
void moveAfter(Operation *existingOp);
void moveAfter(Block *block, llvm::iplist<Operation>::iterator iterator);
```



## mlir::Value

Value可以理解成操作数，参数等等，如例子中的`%arg0`、`%1`。

Value几乎所有接口都非常有用：

``` c++
Type getType() const;
void setType(Type newType);
MLIRContext *getContext() const { return getType().getContext(); }
Location getLoc() const;
Region *getParentRegion();
Block *getParentBlock();
void replaceAllUsesWith(Value newValue) const;
void replaceUsesWithIf(Value newValue, function_ref<bool(OpOperand &)> shouldReplace);
use_range getUses() const { return {use_begin(), use_end()}; }
bool hasOneUse() const;
bool use_empty() const;
void dump();
```

#### BlockArgument

如例子中的`%arg0`、`%arg1`等等

```c++
class BlockArgument : public Value{
  /// Returns the block that owns this argument.
  Block *getOwner() const { return getImpl()->owner; }

  /// Return the type of this value.
  Type getType() const { return getImpl()->type; }

  /// Set the type of this value.
  void setType(Type newType) { getImpl()->type = newType; }

  /// Returns the number of this argument.
  unsigned getArgNumber() const { return getImpl()->index; }
}
```

#### OpResult

如例中的`%0`、`%1`、`%2`等等

``` c++
class OpResult : public Value {
public:
  using Value::Value;

  static bool classof(Value value) {
    return value.getKind() != Kind::BlockArgument;
  }

  /// Returns the operation that owns this result.
  Operation *getOwner() const;

  /// Returns the number of this result.
  unsigned getResultNumber() const;
}
```

通过value，获取operation，如下：

```c++
// %2对应value，则
Operation * operation = value.getDefiningOp();
toy::MulOp op = value.getDefiningOp<toy::MulOp>();
// get Value from operation
Value inner_value = operation->getResult(0);
assert(inner_value == value);
```



## mlir::Type

Type可以理解成Value的类型，如例子中的`tensor<*xf64>`、`tensor<2x3xf64>`等等。



## Block

Block是指一系列operation的集合，包括输入和输出，语法如下：

```
block           ::= block-label operation+
block-label     ::= block-id block-arg-list? `:`
block-id        ::= caret-id
caret-id        ::= `^` suffix-id
value-id-and-type ::= value-id `:` type

// Non-empty list of names and types.
value-id-and-type-list ::= value-id-and-type (`,` value-id-and-type)*

block-arg-list ::= `(` value-id-and-type-list? `)`
```





## Dialect定义

``` c#
def ONNXAddOp:ONNX_Op<"Add",
  [NoSideEffect, DeclareOpInterfaceMethods<ShapeInferenceOpInterface>]> {
  let hasCanonicalizer = 1;
  let summary = "ONNX Add operation";
  let description = [{
  "Performs element-wise binary addition (with Numpy-style broadcasting support)."
  ""
  "This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md)."
  }];
  let arguments = (ins AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$A,
    AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$B);
  let results = (outs AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$C);
  let builders = [
    OpBuilder<(ins "Value":$A, "Value":$B), [{
      auto lhsTy = A.getType();
      auto rhsTy = B.getType();
      auto elementType = getBroadcastedRankedType(lhsTy, rhsTy);
      auto shapedType = elementType.dyn_cast_or_null<ShapedType>();
      if (!shapedType || !shapedType.hasStaticShape()) {
            elementType = A.getType().cast<ShapedType>().getElementType();
            elementType = UnrankedTensorType::get(elementType);
      }
      build($_builder, $_state, elementType, A, B);
    }]>,
    OpBuilder<(ins "ValueRange":$operands, "ArrayRef<NamedAttribute>":$attributes), [{
      auto lhsTy = operands[0].getType();
      auto rhsTy = operands[1].getType();
      auto elementType = getBroadcastedRankedType(lhsTy, rhsTy);
      auto shapedType = elementType.dyn_cast_or_null<ShapedType>();
      if (!shapedType || !shapedType.hasStaticShape()) {
            elementType = operands[0].getType().cast<ShapedType>().getElementType();
            elementType = UnrankedTensorType::get(elementType);
      }
      std::vector<mlir::Type> outputTypes;
      outputTypes.emplace_back(elementType);
      build($_builder, $_state, outputTypes, operands, attributes);
    }]>
    ];
    let extraClassDeclaration = [{
      static int getNumberOfOperands() {
        return 2;
      }
      static int getNumberOfResults() {
        return 1;
      }
      static std::vector<int> getTypeMap() {
        return {20};
      }
    }];
}
```

#### ShapeInferenceOpInterface

定义shape推导接口，Dialect定义如下：

``` json
def ShapeInferenceOpInterface : OpInterface<"ShapeInference"> {
  let methods = [
    InterfaceMethod<"Infer and set the output shape for the current operation.",
                    "LogicalResult", "inferShapes",
                    (ins "std::function<void(mlir::Region&)>":$shapeInferenceFunc)>
  ];
}
```

也就是需要op定义一个inferShapes接口，ONNXAddOp的对应接口如下：

``` c++
LogicalResult ONNXAddOp::inferShapes(
    std::function<void(mlir::Region &)> doShapeInference) {
  if (!getOperand(0).getType().isa<RankedTensorType>() ||
      !getOperand(1).getType().isa<RankedTensorType>())
    return success();
  auto lhsTy = getOperand(0).getType().cast<RankedTensorType>();
  auto rhsTy = getOperand(1).getType().cast<RankedTensorType>();
  getResult().setType(getBroadcastedType(lhsTy, rhsTy));
  return success();
}
```







