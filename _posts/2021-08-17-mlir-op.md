---
layout: single
title: MLIR的细节整理
categories:
  - AI
tags:
  - Mlir
---

* content
{:toc}


以一段来自官网[toy](https://mlir.llvm.org/docs/Tutorials/Toy/Ch-2/)，来解释各种概念

```python
# User defined generic function that operates on unknown shaped arguments.
def multiply_transpose(a, b) {
  return transpose(a) * transpose(b);
}

def main() {
  var a<2, 3> = [[1, 2, 3], [4, 5, 6]];
  var b<2, 3> = [1, 2, 3, 4, 5, 6];
  var c = multiply_transpose(a, b);
  var d = multiply_transpose(b, a);
  print(d);
}
```

转成IR定义如下：

<!--more-->

```json
module {
  func @multiply_transpose(%arg0: tensor<*xf64>, %arg1: tensor<*xf64>) -> tensor<*xf64> {
    %0 = "toy.transpose"(%arg0) : (tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:10)
    %1 = "toy.transpose"(%arg1) : (tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
    %2 = "toy.mul"(%0, %1) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
    "toy.return"(%2) : (tensor<*xf64>) -> () loc("Toy/Ch2/codegen.toy":5:3)
  } loc("Toy/Ch2/codegen.toy":4:1)
  func @main() {
    %0 = "toy.constant"() {value = dense<[[1.000000e+00, 2.000000e+00, 3.000000e+00], [4.000000e+00, 5.000000e+00, 6.000000e+00]]> : tensor<2x3xf64>} : () -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":9:17)
    %1 = "toy.reshape"(%0) : (tensor<2x3xf64>) -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":9:3)
    %2 = "toy.constant"() {value = dense<[1.000000e+00, 2.000000e+00, 3.000000e+00, 4.000000e+00, 5.000000e+00, 6.000000e+00]> : tensor<6xf64>} : () -> tensor<6xf64> loc("Toy/Ch2/codegen.toy":10:17)
    %3 = "toy.reshape"(%2) : (tensor<6xf64>) -> tensor<2x3xf64> loc("Toy/Ch2/codegen.toy":10:3)
    %4 = "toy.generic_call"(%1, %3) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":11:11)
    %5 = "toy.generic_call"(%3, %1) {callee = @multiply_transpose} : (tensor<2x3xf64>, tensor<2x3xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":12:11)
    "toy.print"(%5) : (tensor<*xf64>) -> () loc("Toy/Ch2/codegen.toy":13:3)
    "toy.return"() : () -> () loc("Toy/Ch2/codegen.toy":8:1)
  } loc("Toy/Ch2/codegen.toy":8:1)
} loc(unknown)
```



## mlir::Operation

```json
%2 = "toy.mul"(%0, %1) : (tensor<*xf64>, tensor<*xf64>) -> tensor<*xf64> loc("Toy/Ch2/codegen.toy":5:25)
```

Operation是指操作，也可以理解成运算。如上中`%2 = "toy.mul"(%0,%1)`可以看做一个operation。如toy中的`toy.mul`、`toy.transpose`、`toy.constant`等等都是Operation的名称。

代码中`mlir::Operation`是通用定义，包含通用的接口和属性；`MulOp`、`TransposeOp`、`ConstantOp`等等是特定定义，包含特定的属性。前者可以通过`llvm::dyn_cast`（动态）或`llvm::cast`（静态）转换成后者；后者通过`getOperation()`转换成前者。如下：

``` c++
void processConstantOp(mlir::Operation *operation) {
  ConstantOp op = llvm::dyn_cast<ConstantOp>(operation);

  // This operation is not an instance of `ConstantOp`.
  if (!op)
    return;

  // Get the internal operation instance wrapped by the smart pointer.
  mlir::Operation *internalOperation = op.getOperation();
  assert(internalOperation == operation &&
         "these operation instances are the same");
}
```

特定Op可以直接访问属性；`mlir::Operation`无法直接访问特定Op的属性，但可以类似如下间接访问，如下：

``` c++
// getAttrOfType
IntegerAttr opsetAttr = op->getAttrOfType<::mlir::Attribute>("onnx_opset").dyn_cast_or_null<IntegerAttr>();
if (opsetAttr)
  opset = opsetAttr.getValue().getSExtValue();
// getAttr
mlir::Type targetType = op->getAttr("to").cast<::mlir::TypeAttr>().getValue();
```

`mlir::Operation`有如下常用接口：

```c++
OperationName getName() { return name; }
/// Remove this operation from its parent block and delete it.
void erase();
/// Remove the operation from its parent block, but don't delete it.
void remove();
/// Returns the operation block that contains this operation.
Block *getBlock() { return block; }
/// Return the context this operation is associated with.
MLIRContext *getContext();
void dump();
unsigned getNumOperands();
Value getOperand(unsigned idx) { return getOpOperand(idx).get(); }
void setOperand(unsigned idx, Value value);
unsigned getNumResults();
/// Get the 'idx'th result of this operation.
OpResult getResult(unsigned idx) { return OpResult(this, idx); }
void replaceAllUsesWith(Operation *op);
void moveBefore(Operation *existingOp);
void moveBefore(Block *block, llvm::iplist<Operation>::iterator iterator);
void moveAfter(Operation *existingOp);
void moveAfter(Block *block, llvm::iplist<Operation>::iterator iterator);
```



## mlir::Value

Value可以理解成操作数，参数等等，如例子中的`%arg0`、`%1`。

Value几乎所有接口都非常有用：

``` c++
Type getType() const;
void setType(Type newType);
MLIRContext *getContext() const { return getType().getContext(); }
Location getLoc() const;
Region *getParentRegion();
Block *getParentBlock();
void replaceAllUsesWith(Value newValue) const;
void replaceUsesWithIf(Value newValue, function_ref<bool(OpOperand &)> shouldReplace);
use_range getUses() const { return {use_begin(), use_end()}; }
bool hasOneUse() const;
bool use_empty() const;
void dump();
```

#### BlockArgument

如例子中的`%arg0`、`%arg1`等等

```c++
class BlockArgument : public Value{
  /// Returns the block that owns this argument.
  Block *getOwner() const { return getImpl()->owner; }

  /// Return the type of this value.
  Type getType() const { return getImpl()->type; }

  /// Set the type of this value.
  void setType(Type newType) { getImpl()->type = newType; }

  /// Returns the number of this argument.
  unsigned getArgNumber() const { return getImpl()->index; }
}
```

#### OpResult

如例中的`%0`、`%1`、`%2`等等

``` c++
class OpResult : public Value {
public:
  using Value::Value;

  static bool classof(Value value) {
    return value.getKind() != Kind::BlockArgument;
  }

  /// Returns the operation that owns this result.
  Operation *getOwner() const;

  /// Returns the number of this result.
  unsigned getResultNumber() const;
}
```

通过value，获取operation，如下：

```c++
// %2对应value，则
Operation * operation = value.getDefiningOp();
toy::MulOp op = value.getDefiningOp<toy::MulOp>();
// get Value from operation
Value inner_value = operation->getResult(0);
assert(inner_value == value);
```

## mlir::Block

## mlir::Region



## mlir::Type

Type可以理解成Value的类型，如例子中的`tensor<*xf64>`、`tensor<2x3xf64>`等等对应的是TensorType，继承自`mlir::Type`。

type常用接口有这些：

``` c++
bool operator==(Type other) const { return impl == other.impl; }
bool operator!=(Type other) const { return !(*this == other); }
explicit operator bool() const { return impl; }
bool operator!() const { return impl == nullptr; }
template <typename U> bool isa() const;
template <typename First, typename Second, typename... Rest> bool isa() const;
template <typename U> U dyn_cast() const;
template <typename U> U dyn_cast_or_null() const;
template <typename U> U cast() const;  
bool isIndex() const;
bool isBF16() const;
bool isF16() const;
bool isF32() const;
bool isF64() const;
bool isF80() const;
bool isF128() const;

/// Return true if this is an integer type with the specified width.
bool isInteger(unsigned width) const;
```

#### ShapeType

ShapeType用于表示Shape，有`ranked`和`unranked`之分，ranked在维度上又有`static`和`dynamic`之分。

有如下这些方式：

* `[*]` if it is an unranked shape
* `[?, 2]` if a rank 2 tensor with one unknown dimension
* `[3, 4]` is a rank 2 static tensor
* `[]` is a scalar
* `[1]` is a rank 1 tensor with 1 element
* `[invalid]` for an invalid shape

有如下这些接口：

```c++
/// Returns whether the shape has a rank.
bool hasRank() const;
/// Returns the element type.
Type getElementType() const;
/// Populates the dimensions from shape referenced.
/// Requires: shape is ranked.
void getDims(SmallVectorImpl<int64_t> &res) const;
/// Populates the dimensions of the ShapeTypeComponents.
/// Requires: shape is ranked.
void getDims(ShapedTypeComponents &res) const;
/// Returns the size of the index'th dimension.
/// Requires: shape is ranked.
int64_t getDimSize(int index) const;
/// Returns whether the index'th dimension is dynamic.
/// Requires: shape is ranked.
bool isDynamicDim(int index) const { // 判断dimSize是否为kDynamicSize(对应-1)
  return ShapedType::isDynamic(getDimSize(index));
}
/// Returns whether the shape is fully static.
bool hasStaticShape() const;
/// Returns the rank of the shape.
/// Requires: shape is ranked.
int64_t getRank() const;
/// Returns the number of elements in the shape.
/// Requires: hasStaticShape
int64_t getNumElements() const;
```



#### TensorType

如例子中的`tensor<*xf64>`、`tensor<2x3xf64>`等等，是type与ShapeType的子类。

定义如下：

``` c++
class TensorType : public Type, public ShapedType::Trait<TensorType> {
public:
  using Type::Type;

  /// Returns the element type of this tensor type.
  Type getElementType() const;

  /// Returns if this type is ranked, i.e. it has a known number of dimensions.
  bool hasRank() const;

  /// Returns the shape of this tensor type.
  ArrayRef<int64_t> getShape() const;

  /// Clone this type with the given shape and element type. If the
  /// provided shape is `None`, the current shape of the type is used.
  TensorType cloneWith(Optional<ArrayRef<int64_t>> shape,
                       Type elementType) const;

  /// Return true if the specified element type is ok in a tensor.
  static bool isValidElementType(Type type);

  /// Methods for support type inquiry through isa, cast, and dyn_cast.
  static bool classof(Type type);

  /// Allow implicit conversion to ShapedType.
  operator ShapedType() const { return cast<ShapedType>(); }
};
```

通过value获取相关信息如下：

``` c++
mlir::Type type = value.getType(); // get mlir::Type
mlir::TensorType tensor_type = type.cast<mlir::TensorType>(); // cast to tensor type
ArrayRef<int64_t> shape = tensor_type.getShape(); // get shape
mlir::Type eleType = tensor_type.getElementType();
assert(eleType.isF64() == true);
```



#### RankedTensorType 与 UnrankedTensorType

如`tensor<*xfp32>`，对应的是UnrankedTensorType；RankedTensorType有如下这些形式：

```json
// Known rank but unknown dimensions.
tensor<? x ? x ? x ? x f32>
// Partially known dimensions.
tensor<? x ? x 13 x ? x f32>
// Full static shape.
tensor<17 x 4 x 13 x 4 x f32>
// Tensor with rank zero. Represents a scalar.
tensor<f32>
// Zero-element dimensions are allowed.should be optimized away before lowering tensors to vectors
tensor<0 x 42 x f32>
```

使用方法举例：

``` c++
// create
mlir::Type getType(ArrayRef<int64_t> shape) {
  // If the shape is empty, then this type is unranked.
  if (shape.empty())
    return mlir::UnrankedTensorType::get(builder.getF64Type());

  // Otherwise, we use the given shape.
  return mlir::RankedTensorType::get(shape, builder.getF64Type());
}
```



## mlir::MLIRContext

mlir上下文，可以理解成一系列对象的最顶层。所有mlir相关对象都依赖MLIRContext。

## mlir::Builder

用于创建全局对象的建造者，比如创建types、attributes等等，声明如下：

``` c++
class Builder {
public:
  explicit Builder(MLIRContext *context) : context(context) {}
  explicit Builder(Operation *op) : Builder(op->getContext()) {}

  MLIRContext *getContext() const { return context; }

  // Locations.
  Location getUnknownLoc();
  // Types.
  FloatType getBF16Type();
  FloatType getF16Type();
  FloatType getF32Type();
  FloatType getF64Type();
  IntegerType getIntegerType(unsigned width);
  IntegerType getIntegerType(unsigned width, bool isSigned);
  // Attributes.
  NamedAttribute getNamedAttr(StringRef name, Attribute val);
  UnitAttr getUnitAttr();
  BoolAttr getBoolAttr(bool value);
  DictionaryAttr getDictionaryAttr(ArrayRef<NamedAttribute> value);
  IntegerAttr getIntegerAttr(Type type, int64_t value);
  IntegerAttr getIntegerAttr(Type type, const APInt &value);
  FloatAttr getFloatAttr(Type type, double value);
  FloatAttr getFloatAttr(Type type, const APFloat &value);
  StringAttr getStringAttr(const Twine &bytes);
  ArrayAttr getArrayAttr(ArrayRef<Attribute> value);
  // Convenience methods for fixed types.
  FloatAttr getF16FloatAttr(float value);
  FloatAttr getF32FloatAttr(float value);
  FloatAttr getF64FloatAttr(double value);
  IntegerAttr getI8IntegerAttr(int8_t value);
  IntegerAttr getI16IntegerAttr(int16_t value);
  IntegerAttr getI32IntegerAttr(int32_t value);
  ArrayAttr getI32ArrayAttr(ArrayRef<int32_t> values);
  ArrayAttr getF32ArrayAttr(ArrayRef<float> values);
```

#### OpBuilder

继承Builder，用于创建Operation。

有如下接口：

```c++
/// Create an operation of specific op type at the current insertion point.
template <typename OpTy, typename... Args>
OpTy create(Location location, Args &&...args) {
  OperationState state(location,
  getCheckRegisteredInfo<OpTy>(location.getContext()));
  OpTy::build(*this, state, std::forward<Args>(args)...);
  auto *op = createOperation(state);
  auto result = dyn_cast<OpTy>(op);
  assert(result && "builder didn't return the right type");
  return result;
}
```

以`onnx-mlir`的一段代码为例，创建OP如下：

``` c++
auto gemmOp = builder.create<ONNXGemmOp>(UnknownLoc::get(&ctx),
                                         /*Y=*/yType, /*A=*/aVal, /*B=*/bVal, /*C=*/cVal, alphaAttr, betaAttr,
                                         aTransAttr, bTransAttr);
gemmOp.getResult().setType(yType);

llvm::SmallVector<Value, 1> results = {gemmOp.getResult()};
builder.create<ReturnOp>(UnknownLoc::get(&ctx), results);
```





## Dialect定义

``` c#
def ONNXAddOp:ONNX_Op<"Add",
  [NoSideEffect, DeclareOpInterfaceMethods<ShapeInferenceOpInterface>]> {
  let hasCanonicalizer = 1;
  let summary = "ONNX Add operation";
  let description = [{
  "Performs element-wise binary addition (with Numpy-style broadcasting support)."
  ""
  "This operator supports **multidirectional (i.e., Numpy-style) broadcasting**; for more details please check [the doc](Broadcasting.md)."
  }];
  let arguments = (ins AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$A,
    AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$B);
  let results = (outs AnyTypeOf<[TensorOf<[UI32]>, TensorOf<[UI64]>, TensorOf<[I32]>, TensorOf<[I64]>, TensorOf<[F16]>, TensorOf<[F32]>, TensorOf<[F64]>, TensorOf<[BF16]>, AnyMemRef]>:$C);
  let builders = [
    OpBuilder<(ins "Value":$A, "Value":$B), [{
      auto lhsTy = A.getType();
      auto rhsTy = B.getType();
      auto elementType = getBroadcastedRankedType(lhsTy, rhsTy);
      auto shapedType = elementType.dyn_cast_or_null<ShapedType>();
      if (!shapedType || !shapedType.hasStaticShape()) {
            elementType = A.getType().cast<ShapedType>().getElementType();
            elementType = UnrankedTensorType::get(elementType);
      }
      build($_builder, $_state, elementType, A, B);
    }]>,
    OpBuilder<(ins "ValueRange":$operands, "ArrayRef<NamedAttribute>":$attributes), [{
      auto lhsTy = operands[0].getType();
      auto rhsTy = operands[1].getType();
      auto elementType = getBroadcastedRankedType(lhsTy, rhsTy);
      auto shapedType = elementType.dyn_cast_or_null<ShapedType>();
      if (!shapedType || !shapedType.hasStaticShape()) {
            elementType = operands[0].getType().cast<ShapedType>().getElementType();
            elementType = UnrankedTensorType::get(elementType);
      }
      std::vector<mlir::Type> outputTypes;
      outputTypes.emplace_back(elementType);
      build($_builder, $_state, outputTypes, operands, attributes);
    }]>
    ];
    let extraClassDeclaration = [{
      static int getNumberOfOperands() {
        return 2;
      }
      static int getNumberOfResults() {
        return 1;
      }
      static std::vector<int> getTypeMap() {
        return {20};
      }
    }];
}
```

#### ShapeInferenceOpInterface

定义shape推导接口，Dialect定义如下：

``` json
def ShapeInferenceOpInterface : OpInterface<"ShapeInference"> {
  let methods = [
    InterfaceMethod<"Infer and set the output shape for the current operation.",
                    "LogicalResult", "inferShapes",
                    (ins "std::function<void(mlir::Region&)>":$shapeInferenceFunc)>
  ];
}
```

也就是需要op定义一个inferShapes接口，ONNXAddOp的对应接口如下：

``` c++
LogicalResult ONNXAddOp::inferShapes(
    std::function<void(mlir::Region &)> doShapeInference) {
  if (!getOperand(0).getType().isa<RankedTensorType>() ||
      !getOperand(1).getType().isa<RankedTensorType>())
    return success();
  auto lhsTy = getOperand(0).getType().cast<RankedTensorType>();
  auto rhsTy = getOperand(1).getType().cast<RankedTensorType>();
  getResult().setType(getBroadcastedType(lhsTy, rhsTy));
  return success();
}
```







