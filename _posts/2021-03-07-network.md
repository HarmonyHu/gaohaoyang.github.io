---
layout: single
title: 常见神经网络特征
categories:
  - AI
tags:
  - 网络模型
---

* content
{:toc}

## VGG16

13层卷积+3层全连接，穿插pooling和relu

其中卷积的kernel均采用3x3

<!--more-->
## ResNet50

解决问题：网络层次深后，梯度消失，难以训练。

加入残差运算，如下：

![](https://harmonyhu.github.io/img/residual.png)

由49层卷积和1层全连接

[ResNet](https://arxiv.org/pdf/1512.03385.pdf)

## GoogleNet

引入LRN运算，实际效果并不好



## ShuffleNet

引入Group Conv，和ShuffleChannel运算



## ResNext50

引入cardinality，可以理解为卷积分组后残差，使网络细化。类似`multi-head attention`。

![](https://harmonyhu.github.io/img/resnext.png)

[ResNeXt](https://arxiv.org/pdf/1611.05431.pdf)



## MobileNet

使用`Group Conv`，乃至`Depthwise Conv`，和`Pointwise Conv`，替换Conv。是模型参数大幅减少。

V1是在VGG16上修改，V2是在ResNet50上修改。

#### Pointwise Conv

1x1卷积，kernel为[oc, ic, 1, 1]，可以使输入channel从ic转变为oc，既可以降维，又可升维。

一般通过降维，减少后续layer的参数

#### Group Conv

kernel为[g, oc/g, ic/g, kh, kw]，参数量为 oc * ic * kh * kw / g

深度卷积时g = ic = oc，kernel对应为[ic, 1, 1, kh, kw]，参数量为 oc * kh * kw

正常卷积核为[oc, ic, kh, kw]，参数量为oc * ic * kh * kw

#### 如何减少参数

正常卷积，通过[oc, ic, kh, kw]，将输入转化为[n, oc, oh, ow]，参数量为`oc * ic * kh * kw`；

先1x1卷积[oc, ic, 1, 1]，然后深度卷积[oc, 1, 1, kh, kw]可以达到一样的输出，参数量为`oc * ic + oc * kh * kw`



