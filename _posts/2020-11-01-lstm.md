---
layout: post
title: RNN与LSTM
categories: AI
tags: 算法
---

* content
{:toc}
## RNN

RNN：Recurrent Neural Networks，循环神经网络，能够应用于空间或时间先后相关的场景，比如文字解析、语音识别。

RNN模型结构简单描述，如下图：

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/rnn.jpg)

其中X<sub>0</sub>、X<sub>1</sub>、......、X<sub>t</sub>，可以理解成多个输入，或者对单个输入拆分成的多个输入，比如一张图片的多个字符拆分、语音的拆分等等；h<sub>0</sub>、h<sub>1</sub>、&hellip;、h<sub>t</sub>可以理解成多个输出，通常也可能最终被concat到一起，做为一个输入。

上图是单向的，也就是h<sub>1</sub>会受h<sub>0</sub>的影响，h<sub>2</sub>会受h<sub>1</sub>、h<sub>0</sub>的影响，&hellip;，但反过来不会。RNN网络也会有双向的情况，使前后互相影响。

<!--more-->



## LSTM

LSTM：Long Short Term Memory，长短期记忆神经网络，RNN中的一类，用来避免长期依赖问题，被`Alex Graves`改良和推广。

LSTM模型结构简单描述，如下图：

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/lstm.jpg)

每个LSTM单元有两个状态传递：C<sub>t</sub> (细胞状态) 和h<sub>t</sub> (隐藏状态)。通常C<sub>t</sub>的更新比较慢，h<sub>t</sub>更新比较快。

LSTM可以拆分成4个步骤fico： forget gate、input gate、cell、output gate。

#### 遗忘门 (forget gate)

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/lstm_f.jpg)

&sigma;是sigmoid函数，产生0到1直接的数值，决定C<sub>t-1</sub>更新多少，可以理解成丢弃多少，所以称作遗忘门。

实际应用中，C<sub>0</sub>和h<sub>0</sub>可以是全0向量(caffe)，也可能是权重(onnx)。

#### 输入门 (input gate)

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/lstm_i.jpg)

i<sub>t</sub>也是由sigmoid函数更新，决定对输入更新多少;
$$
\widetilde{C}_{t}
$$
由tanh函数实现，决定有多少数据会添加到细胞状态中。

#### 细胞状态更新 (cell)

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/lstm_c.jpg)

通过前面的遗忘门和输入门，对细胞状态更新C<sub>t-1</sub>到C<sub>t</sub>，作为一个单元的细胞状态输入。

#### 输出门 (output gate)

![](https://github.com/HarmonyHu/harmonyhu.github.io/raw/master/_posts/images/lstm_o.jpg)

其中h<sub>t</sub>即是本层的输出，也是下一个单元的隐藏输入。

由于LSTM参数很多，训练难度大，通常会使用参数更少的GRU。



## 学习文章

<http://colah.github.io/posts/2015-08-Understanding-LSTMs>

